#+TITLE: Probability
#+AUTHOR: Felix Chern
#+DESCRIPTION: Introduction to some elementary probability concepts

* Probability model core concepts

A probability model can split into three compoents:
- Sample space: a set whose elements are called sample points or outcomes
- Events: class of all subsets of the sample space
- Probability measure: can be treat as a relative "volume" of events in the sample space.

** Sample space

A sample space is a set of all the possible /outcomes/ or /sample points/, denoted as \Omega. An outcome \omega is often called a finest grain result when even {\omega} contains no proper subsets. The outcomes in the sample space are /mutually exclusive/ and /collectively constitute/ the entire sample space.

** Events

Given a sample space \Omega, the class of subsets of \Omega that constitute the set of events satisfies the followings:

1. \Omega is an event.
2. For every sequence of events A_1, A_2, ..., the union \cup_{n=1}^\infty A_n is an event
3. For every event A, the complement A^c is an event

From 1. and 3., we can see \empty is also an event since \Omega^c = \empty.

** Probability

Given any sample space \Omega and any class of events \Epsilon, a probability rule is a function Pr{\cdot} mapping each A \in \Epsilon to a real number in such the following holds:

1. Pr{\Omega} = 1
2. For every event A, Pr{A} \ge 0
3. The probability of the union of any sequence A_1, A_2,... of disjoint events is given by
Pr{\cup_{n=1}^\infty A_n} = \sum_{n=1}^\infty Pr{A_n}

These can imply the followings:

- Pr{\empty} = 0
- Pr{\cup_{n=1}^m A_n} = \sum_{n=1}^m Pr{A_n} for A_1,...,A_m disjoint
- Pr{A^c} = 1 - Pr{A} for all A
- Pr{A} \le Pr{B} for all A \sube B
- Pr{A} \le 1 for all A
- \sum_n Pr{A_n} \le 1 for A_1,..,A_n disjoint

* Probability theories

** Conditional probabilities

For any two events A and B in a probability model, the conditional probability of A, conditional on B is defined if Pr{B} > 0 by

Pr{A|B} = Pr{AB}/Pr{B}

Bayes' law can be written in the form

Pr{A|B}Pr{B} = Pr{B|A}Pr{A}

which directly derived from the conditional definition.

** Statistical independent

Two events, A and B, are statistically independent if

Pr{AB} = Pr{A}Pr{B}

For Pr{B} > 0, this is equivalent to Pr{AB} = Pr{A}.

*** conditionally independent
Given C if Pr{AB|C} = Pr{A|C}Pr{B|C}

*** Multiple event indepedence
The events A_1,...,A_n, n > 2 are statistically independent if for each /collection of S for two or more of the integers 1 to n/.

\[
Pr\left\{\bigcap_{i\in S}A_{i }\right\} = \prod_{i\in S}Pr\left\{ A_i \right\}
\]

Note S is a power set of A_1,...,A_n excluding \empty and individuals.

** Random variables

A random variable rv is a function X from the sample space \Omega to the set of real numbers \real. Three modifications are made.

1. X might be undefined or infinite for a subset of \Omega that has 0 probability.
2. The mapping X(\omega) must have the property that {\omega \in \Omega : X(\omega) \le x} is an event.
3. Every finite set of rv s X_1,...,X_n has the property that for each x_1 \in \real,...,x_n \in \real, the set {\omega:X_1(\omega) \le x_1,...,X_n(\omega) \le x_n} is an event.

We need 2 and 3 else the cumulative distribution function may not exists.

A random variable can be thought as a way to pick events from \Omega. Even if there are infinite events we just need the cumulative form {\omega \in \Omega : X(\omega) \le x}, and the volume relative to \Omega can be extracted by Pr{\cdot} function.

** Cumulative distribution function (CDF)

The cumulative distribution function of a rv X is a function F_X(x) mapping each x \in \real into 
F_X(x) = Pr{\omega \in \Omega : X(\omega) \le x}. We often omit \omega and write F_X(x) = Pr{X \le x}.

\[
\lim_{x \to -\infty}F_X(x) = 0
\]

\[
\lim_{x \to \infty }F_X(x) = 1
\]


*** Probability mass function (PMF)

If X has only a finite or countable number value x_i, the probability Pr{X = x_i} of each sample x_i is called the probability mass function, denoted by p_X(x_i).

*** Probability density function (PDF)

If the CDF F_X(x) of a rv X has a finite derivative at x, the derivative is called the density, or more precisely the probability density function of X at x is denoted by f_X(x); for \delta > 0 sufficiently small, f_X(x)\delta then approximates the probability that X is mapped to a value between x and x + \delta.

A rv is said to be continuous if there is a funciton f_X(x) such that, for each x \in \real, the CDF satisfies 

\[
F_X(x) = \int_{-\infty}^x f_X(y)dy
\]


* Common distributions

** Binomial distribution

#+name: binomial
#+header: :var n=40
#+BEGIN_SRC python :results value :export none
def binom(p,o):
  o[1] = n*p*(1-p)**(n-1)
  for k in range(2,n+1):
    o[k] = o[k-1] * (n+1-k) / k * p / (1-p)
p1, p3, p5, p7, p9 = 0.1, 0.3, 0.5, 0.7, 0.9
o1 = [0.0]*(n+1)
o3 = [0.0]*(n+1)
o5 = [0.0]*(n+1)
o7 = [0.0]*(n+1)
o9 = [0.0]*(n+1)
binom(p1,o1)
binom(p3,o3)
binom(p5,o5)
binom(p7,o7)
binom(p9,o9)

return zip(o1,o3,o5,o7,o9)
#+END_SRC


#+BEGIN_SRC gnuplot :var data=binomial :file img/prob-001.png :exports both :term png small size 480,320
p data u 1 w histeps tit "p=0.1", \
  data u 2 w histeps tit "p=0.3", \
  data u 3 w histeps tit "p=0.5", \
  data u 4 w histeps tit "p=0.7", \
  data u 5 w histeps tit "p=0.9"
#+END_SRC

#+RESULTS:
[[file:img/prob-001.png]]


** Kullback-Liebler divergence

\[
D_{KL}(P||Q) = \sum_{i}P(i)log \frac{P(i)}{Q(i)}
\]

#+NAME: kl-divergence
#+HEADER: :var n=20
#+BEGIN_SRC python
from math import log
def DKL(p,n,k):
  p_t = float(k)/float(n)
  return p_t*log(p_t/p) + (1-p_t)*log((1-p_t)/(1-p))
xrange = range(1,n)
return zip(xrange, 
  map(lambda k: DKL(0.1,n,k), xrange),
  map(lambda k: DKL(0.3,n,k), xrange),
  map(lambda k: DKL(0.5,n,k), xrange),
  map(lambda k: DKL(0.7,n,k), xrange),
  map(lambda k: DKL(0.9,n,k), xrange)
)
#+END_SRC

#+BEGIN_SRC gnuplot :var data=kl-divergence :file img/prob-002.png :exports both :term png small size 480,320
p data u 1:2 w histeps tit "p=0.1", \
  data u 1:3 w histeps tit "p=0.3", \
  data u 1:4 w histeps tit "p=0.5", \
  data u 1:5 w histeps tit "p=0.7", \
  data u 1:6 w histeps tit "p=0.9"
#+END_SRC

#+RESULTS:
[[file:img/prob-002.png]]

** Moment generating function

\[
M(t) = g_X(t) = E\left[e^{tX}\right] = \int_{-\infty}^\infty e^{tX}dF_X(x)
\]

The k_{th} derivative of g_X(0) is the k_{th} moment of X: E[X^{k}].

The logrithm of the MGF, called cumulant gnerating function is also useful.

\[
R(t) = \log\left[M(t)\right]
\]

\[
R'(t) = \frac{M'(t)}{M(t)}
\]

\[
R''(t) = \frac{M(t)M''(t) - [M'(t)]^2}{[M(t)]^2}
\]

And since M(0) = 1

\[
\mu = M'(0) = R'(0)
\]

and

\[
\sigma^2 = M''(0) - [M'(0)]^2 = R''(0)
\]

*** Binary MGF

#+BEGIN_SRC gnuplot :file img/prob-003.png :exports both :term png small size 480,320
set title "Binary MGF 1-p+pe^r"
p [-0.1:0.1] f(x)=(1-p+p*exp(x)), \
  p=0.1, f(x) tit "p=0.1", \
  p=0.3, f(x) tit "p=0.3", \
  p=0.5, f(x) tit "p=0.5", \
  p=0.7, f(x) tit "p=0.7", \
  p=0.9, f(x) tit "p=0.9"
#+END_SRC

#+RESULTS:
[[file:img/prob-003.png]]

*** Binomial MGF

#+BEGIN_SRC gnuplot :file img/prob-004.png :exports both :term png small size 480,320
set title "Binomial MGF (1-p+pe^r)^n, n=20"
p [-0.1:0.1] f(x)=(1-p+p*exp(x))**20, \
  p=0.1, f(x) tit "p=0.1", \
  p=0.3, f(x) tit "p=0.3", \
  p=0.5, f(x) tit "p=0.5", \
  p=0.7, f(x) tit "p=0.7", \
  p=0.9, f(x) tit "p=0.9"
#+END_SRC

#+RESULTS:
[[file:img/prob-004.png]]

** Chernoff bound

\[
Pr\{S_n \ge na\} \le \left[g_X(r)\right]^n e^{-rna}
\]

#+BEGIN_SRC gnuplot :file img/prob-005.png :exports both :term png small size 480,320
set title "Binomial MGF (1-p+pe^r)^n * exp(-x*na), n=20"
p [-0.3:1.1] f(x)=((1-p+p*exp(x))**20)*exp(-x*a), \
  p=0.5, a=10, f(x) tit "p=0.5, na=10", \
  p=0.5, a=13, f(x) tit "p=0.5, na=13", \
  p=0.5, a=15, f(x) tit "p=0.5, na=15"
#+END_SRC

#+RESULTS:
[[file:img/prob-005.png]]

The smallest value of na=10 is close to 1, while smallest value of na=15 is close to zero. These are the bounds for n=20, Pr{S_n \ge na}.
